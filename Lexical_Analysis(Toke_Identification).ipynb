{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnjwU0/pWmKhz03SMA1rIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bekykm/phd-lowcode-prototypes/blob/main/Lexical_Analysis(Toke_Identification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Define Token Class**\n",
        "\n",
        "**Tokens** can generally include **keywords**, **identifiers**, **operators**, **literals**, and **punctuation**.\n",
        "\n",
        "Create a class to represent tokens.\n"
      ],
      "metadata": {
        "id": "LYaTctaHFUxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aCjzHWcvEYhj"
      },
      "outputs": [],
      "source": [
        "class Token:\n",
        "    # Initialize the token with its type and value\n",
        "    def __init__(self, type: str, value: str):\n",
        "        self.type = type  # Token type (e.g., IDENTIFIER, NUMBER)\n",
        "        self.value = value  # Token value (the actual string)\n",
        "\n",
        "    # String representation of the token for easier debugging\n",
        "    def __repr__(self):\n",
        "        return f'Token({self.type}, {self.value})'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create a Lexer Class**\n",
        "\n",
        "This class will handle tokenization.\n",
        "\n",
        "It will read characters from the input, build tokens, and maintain the current position in the input."
      ],
      "metadata": {
        "id": "8kIeD1fwFl6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lexer:\n",
        "    # Initialize the lexer with the input text\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text  # Save the input text\n",
        "        self.position = 0  # Initialize the current position in the text\n",
        "        # Set the current character based on the initial position\n",
        "        self.current_char = self.text[self.position] if self.text else None\n",
        "        self.tokens = []  # List to hold tokens\n",
        "\n",
        "    # Method to raise an exception for invalid characters\n",
        "    def error(self):\n",
        "        raise Exception('Invalid character')\n",
        "\n",
        "    # Advance the current position and update the current character\n",
        "    def advance(self):\n",
        "        self.position += 1  # Move to the next character\n",
        "        if self.position >= len(self.text):  # Check if the end of input is reached\n",
        "            self.current_char = None  # Set current character to None\n",
        "        else:\n",
        "            self.current_char = self.text[self.position]  # Update the current character\n",
        "\n",
        "    # Skip whitespace characters\n",
        "    def skip_whitespace(self):\n",
        "        while self.current_char is not None and self.current_char.isspace():\n",
        "            self.advance()  # Advance while whitespace is encountered\n",
        "\n",
        "    # Generate an IDENTIFIER token (for variable names)\n",
        "    def id(self):\n",
        "        result = ''  # String to build the identifier\n",
        "        while self.current_char is not None and (self.current_char.isalnum() or self.current_char == '_'):\n",
        "            result += self.current_char  # Append current character to the result\n",
        "            self.advance()  # Move to the next character\n",
        "        return Token('IDENTIFIER', result)  # Return the identifier token\n",
        "\n",
        "    # Generate a NUMBER token (for numeric literals)\n",
        "    def number(self):\n",
        "        result = ''  # String to build the number\n",
        "        while self.current_char is not None and self.current_char.isdigit():\n",
        "            result += self.current_char  # Append current character to the result\n",
        "            self.advance()  # Move to the next character\n",
        "        return Token('NUMBER', result)  # Return the number token\n",
        "\n",
        "    # Generate an OPERATOR token (for operators like +, -, *, /)\n",
        "    def operator(self):\n",
        "        result = self.current_char  # Current character is the operator\n",
        "        self.advance()  # Move to the next character\n",
        "        return Token('OPERATOR', result)  # Return the operator token\n",
        "\n",
        "         # Main tokenize method to parse the input text and generate tokens\n",
        "    def tokenize(self):\n",
        "        while self.current_char is not None:  # Continue until all characters are processed\n",
        "            if self.current_char.isspace():  # If current character is a whitespace\n",
        "                self.skip_whitespace()  # Skip the whitespace\n",
        "                continue  # Continue to the next character\n",
        "            if self.current_char.isalpha():  # If current character is alphabetic\n",
        "                self.tokens.append(self.id())  # Generate an identifier token and append to tokens\n",
        "                continue  # Continue to the next character\n",
        "            if self.current_char.isdigit():  # If current character is a digit\n",
        "                self.tokens.append(self.number())  # Generate a number token and append to tokens\n",
        "                continue  # Continue to the next character\n",
        "            if self.current_char in '+-*/=':  # If current character is an operator (added '=')\n",
        "                self.tokens.append(self.operator())  # Generate an operator token and append to tokens\n",
        "                continue  # Continue to the next character\n",
        "            self.error()  # Raise an error if an invalid character is found\n",
        "        return self.tokens  # Return the list of generated tokens"
      ],
      "metadata": {
        "id": "ZQRZHy3xEfEz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Run the Lexer:** Implement the Main Function\n",
        "\n",
        "Create a simple input string and run the lexer to see the tokens."
      ],
      "metadata": {
        "id": "ZtRl4SZuF4oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the lexer and demonstrate its functionality\n",
        "def main():\n",
        "    text = \"var1 = 10 + 20 * var2\"  # Example input string for the lexer\n",
        "    lexer = Lexer(text)  # Create a Lexer object with the input text\n",
        "    tokens = lexer.tokenize()  # Tokenize the input string\n",
        "    for token in tokens:  # Iterate over the list of tokens\n",
        "        print(token)  # Print each token\n",
        "\n",
        "main()  # Call the main function to execute the lexer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkt6GzbQEjk6",
        "outputId": "2030abed-7d61-4615-eca4-4f42e9f049bd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token(IDENTIFIER, var1)\n",
            "Token(OPERATOR, =)\n",
            "Token(NUMBER, 10)\n",
            "Token(OPERATOR, +)\n",
            "Token(NUMBER, 20)\n",
            "Token(OPERATOR, *)\n",
            "Token(IDENTIFIER, var2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvO8kT8kElpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}